{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75adc9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'202505161637': ('AUC', 1.0, 1e-05, datetime.datetime(2025, 5, 16, 18, 54, 18, 622859)), '202505070419': ('Precision', 1.0, 1e-05, datetime.datetime(2025, 5, 15, 20, 15, 45, 649085)), '202505170232': ('F1-Score', 1.0, 1e-05, datetime.datetime(2025, 5, 17, 5, 26, 3, 436569)), '202505170910': ('Recall', 1.0, 1e-05, datetime.datetime(2025, 5, 17, 15, 36, 10, 805811)), '202505161903': ('AUC', 3.0, 1e-05, datetime.datetime(2025, 5, 17, 1, 21, 4, 994784)), '202505170526': ('F1-Score', 3.0, 1e-05, datetime.datetime(2025, 5, 17, 7, 32, 36, 458159)), '202505150921': ('Precision', 3.0, 1e-05, datetime.datetime(2025, 5, 15, 20, 44, 23, 491416)), '202505150606': ('Recall', 3.0, 1e-05, datetime.datetime(2025, 5, 16, 0, 37, 31, 384408)), '202505150349': ('AUC', 5.0, 1e-05, datetime.datetime(2025, 5, 16, 1, 29, 3, 290500)), '202505170732': ('F1-Score', 5.0, 1e-05, datetime.datetime(2025, 5, 17, 9, 10, 19, 355577)), '202505151015': ('Precision', 5.0, 1e-05, datetime.datetime(2025, 5, 15, 21, 3, 33, 654616)), '202505150652': ('Recall', 5.0, 1e-05, datetime.datetime(2025, 5, 16, 0, 33, 50, 439339))}\n",
      "Evaluating version 202505161637\n",
      "Metric: AUC, Epsilon: 1.0, Delta: 1e-05\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Evaluating version 202505070419\n",
      "Metric: Precision, Epsilon: 1.0, Delta: 1e-05\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Evaluating version 202505170232\n",
      "Metric: F1-Score, Epsilon: 1.0, Delta: 1e-05\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Evaluating version 202505170910\n",
      "Metric: Recall, Epsilon: 1.0, Delta: 1e-05\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Evaluating version 202505161903\n",
      "Metric: AUC, Epsilon: 3.0, Delta: 1e-05\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Evaluating version 202505170526\n",
      "Metric: F1-Score, Epsilon: 3.0, Delta: 1e-05\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Evaluating version 202505150921\n",
      "Metric: Precision, Epsilon: 3.0, Delta: 1e-05\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Evaluating version 202505150606\n",
      "Metric: Recall, Epsilon: 3.0, Delta: 1e-05\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Evaluating version 202505150349\n",
      "Metric: AUC, Epsilon: 5.0, Delta: 1e-05\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Evaluating version 202505170732\n",
      "Metric: F1-Score, Epsilon: 5.0, Delta: 1e-05\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Evaluating version 202505151015\n",
      "Metric: Precision, Epsilon: 5.0, Delta: 1e-05\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Evaluating version 202505150652\n",
      "Metric: Recall, Epsilon: 5.0, Delta: 1e-05\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Set the working directory to the parent directory\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "# Import relevant custom libraries\n",
    "from src.eda import data_info\n",
    "from src.evaluation import ValidationEvaluation\n",
    "\n",
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# Config\n",
    "pd.set_option('display.max_columns', None) # Ensure all columns are displayed\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Read relevant files\n",
    "X_train = pd.read_feather(\"../data/processed/X_train.feather\")\n",
    "X_train_validate = pd.read_feather(\"../data/processed/X_train_validate.feather\")\n",
    "\n",
    "# Get data info\n",
    "var_info = data_info(X_train)\n",
    "all_cols = X_train.columns\n",
    "real_cols = var_info[var_info[\"var_type\"]==\"numerical\"][\"var_name\"].tolist()\n",
    "binary_cols = var_info[var_info[\"var_type\"]==\"binary\"][\"var_name\"].tolist()\n",
    "\n",
    "# Read relevant files\n",
    "X_validate = pd.read_feather(\"../data/processed/X_validate.feather\")\n",
    "y_validate = pd.read_feather(\"../data/processed/y_validate.feather\")\n",
    "\n",
    "# Initialize the validation evaluation\n",
    "valeval = ValidationEvaluation(X_validate, y_validate, real_cols, binary_cols, all_cols, dp_sgd=True)\n",
    "    \n",
    "# Read the log file\n",
    "log_path = \"../logs/dpsgd_tune_log.txt\"\n",
    "\n",
    "# Extract the latest successful Bayesian versions\n",
    "latest_successful_versions = valeval.extract_latest_successful_bayesian_versions(log_path)\n",
    "print(latest_successful_versions)\n",
    "\n",
    "# Evaluate the model performance\n",
    "eval_results = valeval.evaluate_model_performance(latest_successful_versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dede3ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.468514516045322\n"
     ]
    }
   ],
   "source": [
    "from src.dp_utils_poisson import DPSGDSanitizer\n",
    "\n",
    "san = DPSGDSanitizer(len(X_train), 64, 3, 500, 1e-5)\n",
    "print(san.compute_noise_from_eps())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba0727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202505080154\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "202505080250\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "202505112013\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "202505080012\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "os.chdir(\"/Users/trinhha/Documents/VU AMSTERDAM/STUDY/Thesis/Code/\")\n",
    "from src.models import AnomalyDetector\n",
    "\n",
    "X_test = pd.read_feather(\"data/processed/X_test.feather\")\n",
    "\n",
    "for version in eval_results.index.tolist():\n",
    "    print(version)\n",
    "    # Load model and hyperparameters\n",
    "    model = tf.keras.models.load_model(f\"models/baseline/{version}\")\n",
    "    with open(f\"hyperparams/baseline/{version}.pkl\", \"rb\") as f:\n",
    "        params = pickle.load(f)\n",
    "    detector = AnomalyDetector(\n",
    "                model=model,\n",
    "                real_cols=real_cols,\n",
    "                binary_cols=binary_cols,\n",
    "                all_cols=all_cols,\n",
    "                lam=params[\"lam\"],\n",
    "                gamma=params[\"gamma\"],\n",
    "            )\n",
    "    # Compute scores\n",
    "    scores, x_hat = detector._compute_anomaly_scores(X_test, test_set=True)\n",
    "\n",
    "    # Save reconstructed data\n",
    "    pd.DataFrame(x_hat, columns=all_cols).to_feather(f\"experiments/predictions/baseline/{version}_recons.feather\")\n",
    "\n",
    "    # Detect\n",
    "    y_pred = detector._detect(scores, params['threshold'])\n",
    "\n",
    "    # Save predictions\n",
    "    pd.DataFrame(y_pred, columns=[\"anomaly\"]).to_feather(f\"experiments/predictions/baseline/{version}_pred.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4e8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../hyperparams/dpsgd/202506070611.pkl\", \"rb\") as f:\n",
    "    params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7344e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['threshold'] = 0.06004473716020584\n",
    "params['q'] = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b913b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../hyperparams/dpsgd/202506070611.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a95599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "# Import relevant custom libraries\n",
    "from src.eda import data_info\n",
    "from src.evaluation import AnomalyDetector\n",
    "\n",
    "import os\n",
    "os.chdir(\"/Users/trinhha/Documents/VU AMSTERDAM/STUDY/Thesis/Code/\")\n",
    "\n",
    "X_test = pd.read_feather(\"data/processed/X_test.feather\")\n",
    "y_test = pd.read_feather(\"data/processed/y_test.feather\")\n",
    "# Get data info\n",
    "var_info = data_info(X_test)\n",
    "all_cols = X_test.columns\n",
    "real_cols = var_info[var_info[\"var_type\"]==\"numerical\"][\"var_name\"].tolist()\n",
    "binary_cols = var_info[var_info[\"var_type\"]==\"binary\"][\"var_name\"].tolist()\n",
    "\n",
    "with open(\"hyperparams/dpsgd/202506070611.pkl\", \"rb\") as f:\n",
    "    params = pickle.load(f)\n",
    "model = tf.keras.models.load_model(f\"models/dpsgd/202506070611\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfc3e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = AnomalyDetector(\n",
    "                model=model,\n",
    "                real_cols=real_cols,\n",
    "                binary_cols=binary_cols,\n",
    "                all_cols=all_cols,\n",
    "                lam=params[\"lam\"],\n",
    "                gamma=params[\"gamma\"],\n",
    "                target_epsilon=3, delta=1e-5\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347700b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = detector._compute_anomaly_scores(X_test)\n",
    "y_pred = detector._detect(scores, params[\"threshold\"])\n",
    "perf = detector._evaluate(y_pred, y_test, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8f9c720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7446102819237148,\n",
       " 'precision': 0.41311379006174653,\n",
       " 'recall': 0.6017130620985011,\n",
       " 'f1_score': 0.4898884239888424,\n",
       " 'auc': 0.7593880854671883}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7f3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/trinhha/Documents/VU AMSTERDAM/STUDY/Thesis/Code/\")\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from src.models import AutoencoderTrainer, AnomalyDetector\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from math import pi\n",
    "from src.dp_utils import *\n",
    "from src.eda import data_info\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "class StatisticalEval():\n",
    "    def __init__(self):\n",
    "\n",
    "        # Load training data\n",
    "        self.X_train = pd.read_feather(\"data/processed/X_train.feather\")\n",
    "\n",
    "        # Load train-validation data\n",
    "        self.X_train_val = pd.read_feather(\"data/processed/X_train_validate.feather\")\n",
    "        \n",
    "        # Load test data\n",
    "        self.X_test = pd.read_feather(\"data/processed/X_test.feather\")\n",
    "        self.y_test = pd.read_feather(\"data/processed/y_test.feather\")\n",
    "        \n",
    "        # Extract variable types from metadata\n",
    "        self.var_info = data_info(self.X_test)\n",
    "        self.all_cols = self.X_test.columns\n",
    "        self.real_cols = self.var_info[self.var_info[\"var_type\"] == \"numerical\"][\"var_name\"].tolist()\n",
    "        self.binary_cols = self.var_info[self.var_info[\"var_type\"] == \"binary\"][\"var_name\"].tolist()\n",
    "\n",
    "        # Metrics\n",
    "        self.metric_labels = {\n",
    "            \"precision\": \"Precision\",\n",
    "            \"recall\": \"Recall\",\n",
    "            \"f1_score\": \"F1-Score\",\n",
    "            \"auc\": \"AUC\"\n",
    "        }\n",
    "        \n",
    "    def _single_eval(self, model_type, version, epsilon, delta, seed=None):\n",
    "        \n",
    "        with open(f\"hyperparams/{model_type}/{version}.pkl\", \"rb\") as f:\n",
    "            params = pickle.load(f)\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # Train model\n",
    "        trainer = AutoencoderTrainer(\n",
    "            input_dim=self.X_train.shape[1],\n",
    "            real_cols=self.real_cols,\n",
    "            binary_cols=self.binary_cols,\n",
    "            all_cols=self.all_cols,\n",
    "            activation='relu',\n",
    "            patience_limit=10,\n",
    "            verbose=False,\n",
    "            dp_sgd=True if model_type == \"dpsgd\" else False,\n",
    "            post_hoc=False,\n",
    "            target_epsilon=epsilon,\n",
    "            delta=delta,\n",
    "            version=version,\n",
    "            save_tracking=True,\n",
    "            raise_convergence_error=True,\n",
    "            **{key: value for key, value in params.items() if key not in ['threshold', 'q']}\n",
    "        )\n",
    "        model = trainer.train(self.X_train, self.X_train_val)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def final_eval(self, metric_used, seed):\n",
    "\n",
    "        # Load the best models\n",
    "        # Baseline\n",
    "        baseline = pd.read_csv(\"experiments/perf_summary/baseline_val_results.csv\")\n",
    "        baseline_model = baseline.query(f'tuned_by == \"{metric_used}\"')\n",
    "        # DP-SGD\n",
    "        dpsgd = pd.read_csv(\"experiments/perf_summary/dpsgd_val_results.csv\")\n",
    "        dpsgd_models = dpsgd.query(f'tuned_by == \"{metric_used}\"')\n",
    "        \n",
    "        # Load baseline model versions\n",
    "        for version in baseline_model[\"version\"].tolist():\n",
    "            model = self._single_eval(\"baseline\", version, 0, 0, seed)\n",
    "            model.save(f\"models/baseline/{version}_final\")\n",
    "\n",
    "        # Load dpsgd model versions\n",
    "        for i, row in dpsgd_models.iterrows():\n",
    "            epsilon = row[\"epsilon\"]\n",
    "            delta = row[\"delta\"]\n",
    "            version = row[\"version\"]\n",
    "            model = self._single_eval(\"dpsgd\", version, epsilon, delta, seed)\n",
    "            model.save(f\"models/dpsgd/{version}_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1b0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.780134</td>\n",
       "      <td>0.472909</td>\n",
       "      <td>0.687794</td>\n",
       "      <td>0.560461</td>\n",
       "      <td>0.81813</td>\n",
       "      <td>202505080012</td>\n",
       "      <td>2025-06-10 00:25:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  precision    recall  f1_score      auc       version  \\\n",
       "24  0.780134   0.472909  0.687794  0.560461  0.81813  202505080012   \n",
       "\n",
       "              timestamp  \n",
       "24  2025-06-10 00:25:54  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "      <th>version</th>\n",
       "      <th>eps</th>\n",
       "      <th>delta</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.742254</td>\n",
       "      <td>0.411054</td>\n",
       "      <td>0.611563</td>\n",
       "      <td>0.491651</td>\n",
       "      <td>0.755500</td>\n",
       "      <td>202506071334</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2025-06-10 03:54:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.753775</td>\n",
       "      <td>0.425643</td>\n",
       "      <td>0.595717</td>\n",
       "      <td>0.496520</td>\n",
       "      <td>0.764574</td>\n",
       "      <td>202506070611</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2025-06-11 01:29:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.754473</td>\n",
       "      <td>0.430076</td>\n",
       "      <td>0.629550</td>\n",
       "      <td>0.511038</td>\n",
       "      <td>0.788205</td>\n",
       "      <td>202506070329</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2025-06-11 04:26:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     accuracy  precision    recall  f1_score       auc       version  eps  \\\n",
       "108  0.742254   0.411054  0.611563  0.491651  0.755500  202506071334  1.0   \n",
       "107  0.753775   0.425643  0.595717  0.496520  0.764574  202506070611  3.0   \n",
       "106  0.754473   0.430076  0.629550  0.511038  0.788205  202506070329  5.0   \n",
       "\n",
       "       delta            timestamp  \n",
       "108  0.00001  2025-06-10 03:54:53  \n",
       "107  0.00001  2025-06-11 01:29:12  \n",
       "106  0.00001  2025-06-11 04:26:30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Baseline\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>47.512365</td>\n",
       "      <td>69.93576</td>\n",
       "      <td>56.583507</td>\n",
       "      <td>82.496151</td>\n",
       "      <td>140891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  Precision    Recall   F1-Score        AUC    seed\n",
       "50  Baseline  47.512365  69.93576  56.583507  82.496151  140891"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: $\\varepsilon=$5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Fidelity</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>$\\varepsilon=$5</td>\n",
       "      <td>43.729997</td>\n",
       "      <td>64.368308</td>\n",
       "      <td>52.079002</td>\n",
       "      <td>79.178241</td>\n",
       "      <td>84.289081</td>\n",
       "      <td>361524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  Precision     Recall   F1-Score        AUC   Fidelity  \\\n",
       "27  $\\varepsilon=$5  43.729997  64.368308  52.079002  79.178241  84.289081   \n",
       "\n",
       "      seed  \n",
       "27  361524  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: $\\varepsilon=$3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Fidelity</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>$\\varepsilon=$3</td>\n",
       "      <td>44.146855</td>\n",
       "      <td>62.826552</td>\n",
       "      <td>51.855779</td>\n",
       "      <td>78.095394</td>\n",
       "      <td>84.533473</td>\n",
       "      <td>140891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  Precision     Recall   F1-Score        AUC   Fidelity  \\\n",
       "50  $\\varepsilon=$3  44.146855  62.826552  51.855779  78.095394  84.533473   \n",
       "\n",
       "      seed  \n",
       "50  140891  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: $\\varepsilon=$1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Fidelity</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>$\\varepsilon=$1</td>\n",
       "      <td>41.344196</td>\n",
       "      <td>60.856531</td>\n",
       "      <td>49.237699</td>\n",
       "      <td>75.384653</td>\n",
       "      <td>81.984813</td>\n",
       "      <td>58224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  Precision     Recall   F1-Score        AUC   Fidelity  \\\n",
       "35  $\\varepsilon=$1  41.344196  60.856531  49.237699  75.384653  81.984813   \n",
       "\n",
       "     seed  \n",
       "35  58224  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943413\n"
     ]
    }
   ],
   "source": [
    "# Define labels for the metrics to be plotted\n",
    "metric_labels = {\n",
    "            \"precision\": \"Precision\",\n",
    "            \"recall\": \"Recall\",\n",
    "            \"f1_score\": \"F1-Score\",\n",
    "            \"auc\": \"AUC\",\n",
    "            \"fidelity\":  \"Fidelity\"\n",
    "\n",
    "        }\n",
    "metric_used = \"AUC\"\n",
    "\n",
    "# Get the best baseline model\n",
    "baseline = pd.read_csv(\"experiments/perf_summary/baseline_val_results.csv\")\n",
    "baseline_model = baseline.query(f'tuned_by == \"{metric_used}\"')[\"version\"].astype(str).tolist()\n",
    "baseline_test_perf = pd.read_csv(\"results/metrics/baseline.csv\")\n",
    "baseline_test_perf[\"version\"] = baseline_test_perf[\"version\"].astype(str)\n",
    "baseline_best = baseline_test_perf[baseline_test_perf[\"version\"].isin(baseline_model)]\n",
    "display(baseline_best)\n",
    "\n",
    "# Read the test performance of the DP-SGD models\n",
    "dpsgd = pd.read_csv(\"experiments/perf_summary/dpsgd_val_results.csv\")\n",
    "dpsgd_models = dpsgd.query(f'tuned_by == \"{metric_used}\"')[\"version\"].astype(str).tolist()\n",
    "dpsgd_test_perf = pd.read_csv(\"results/metrics/dpsgd.csv\")\n",
    "dpsgd_test_perf[\"version\"] = dpsgd_test_perf[\"version\"].astype(str)\n",
    "dpsgd_best = dpsgd_test_perf[dpsgd_test_perf[\"version\"].isin(dpsgd_models)].sort_values(by=\"eps\", ascending=True)\n",
    "display(dpsgd_best)\n",
    "\n",
    "# Gather the performance metrics and perform normality test\n",
    "model_list = {\"Baseline\": f\"baseline/{m}\" for m in baseline_best[\"version\"].tolist()}\n",
    "model_list.update({r\"$\\varepsilon=$\" + f\"{row['eps']:.0f}\": f\"dpsgd/{row['version']}\" for i, row in dpsgd_best.sort_values(\"eps\", ascending=False).iterrows()})\n",
    "\n",
    "min_len = min([len(pd.read_csv(f\"results/stats_eval/{value}.csv\")) for value in model_list.values()])\n",
    "\n",
    "# Get the seed values\n",
    "seeds = pd.read_csv(\"results/stats_eval/seeds.txt\", header=None).rename(columns={0:\"seed\"})\n",
    "median_ind = int(len(seeds)/2)\n",
    "\n",
    "perf_stats = pd.DataFrame()\n",
    "for key, value in model_list.items():\n",
    "    print(\"Model:\", key)\n",
    "    perf = pd.read_csv(f\"results/stats_eval/{value}.csv\")[:min_len]*100\n",
    "    perf.insert(0, \"Model\", key)\n",
    "    perf[\"seed\"] = seeds\n",
    "    display(perf.sort_values(by=\"AUC\")[median_ind:median_ind+1])\n",
    "\n",
    "    perf_stats = pd.concat([perf_stats, perf], ignore_index=True)\n",
    "\n",
    "mean_by_seed = perf_stats.drop(columns=[\"Model\"]).groupby(\"seed\").mean().sort_values(by=\"AUC\")\n",
    "median_seed = mean_by_seed[median_ind:median_ind+1].index[0]\n",
    "print(median_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29e47ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: models/baseline/202505080012_final/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/baseline/202505080012_final/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/dpsgd/202506071334_final/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/dpsgd/202506071334_final/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/dpsgd/202506070611_final/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/dpsgd/202506070611_final/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/dpsgd/202506070329_final/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/dpsgd/202506070329_final/assets\n"
     ]
    }
   ],
   "source": [
    "# Initialize the evaluation class\n",
    "eval = StatisticalEval()\n",
    "# Run the final evaluation\n",
    "eval.final_eval(metric_used=\"AUC\", seed=int(median_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1bff61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exist_seeds = seeds[\"seed\"].tolist()\n",
    "seed_no = 1\n",
    "while len(exist_seeds) < 100:\n",
    "    random.seed(seed_no)\n",
    "    new_seeds = random.sample(range(1000000), 100 - len(exist_seeds))\n",
    "    no_dup_seeds = [s for s in new_seeds if s not in exist_seeds]\n",
    "    exist_seeds += no_dup_seeds\n",
    "    seed_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "477d3520",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/stats_eval/seeds_new.txt\", \"w\") as f:\n",
    "    for seed in exist_seeds:\n",
    "        f.write(f\"{seed}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec7ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
