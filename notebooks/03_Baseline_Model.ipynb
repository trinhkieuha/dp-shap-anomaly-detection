{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory to the parent directory\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "\n",
    "# Config\n",
    "pd.set_option('display.max_columns', None) # Ensure all columns are displayed\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read relevant files\n",
    "X_train = pd.read_feather(\"../data/processed/X_train.feather\")\n",
    "X_train_validate = pd.read_feather(\"../data/processed/X_train_validate.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_dim, hidden_dims, activation='relu'):\n",
    "    encoder = tf.keras.Sequential()\n",
    "    for h_dim in hidden_dims:\n",
    "        encoder.add(tf.keras.layers.Dense(h_dim, activation=activation,\n",
    "                                          kernel_initializer='glorot_uniform'))\n",
    "\n",
    "    decoder = tf.keras.Sequential()\n",
    "    for h_dim in reversed(hidden_dims[:-1]):\n",
    "        decoder.add(tf.keras.layers.Dense(h_dim, activation=activation,\n",
    "                                          kernel_initializer='glorot_uniform'))\n",
    "    decoder.add(tf.keras.layers.Dense(input_dim, activation='sigmoid',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "    \n",
    "    return encoder, decoder\n",
    "\n",
    "def compute_loss(x, x_hat, model, lam, real_idx, binary_idx):\n",
    "    \n",
    "    # MSE Loss for real-valued features (1/2 factor)\n",
    "    x_real = tf.gather(x, real_idx, axis=1)\n",
    "    x_hat_real = tf.gather(x_hat, real_idx, axis=1)\n",
    "    mse_loss = tf.reduce_sum(0.5 * tf.square(x_real - x_hat_real), axis=1)\n",
    "\n",
    "    # Cross-Entropy Loss for binary-valued features\n",
    "    x_bin = tf.gather(x, binary_idx, axis=1)\n",
    "    x_hat_bin = tf.gather(x_hat, binary_idx, axis=1)\n",
    "    eps = 1e-7  # For numerical stability\n",
    "    x_hat_bin = tf.clip_by_value(x_hat_bin, eps, 1 - eps)\n",
    "    ce_loss = tf.reduce_sum(\n",
    "        -x_bin * tf.math.log(x_hat_bin) - (1 - x_bin) * tf.math.log(1 - x_hat_bin),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Total reconstruction loss (average over batch)\n",
    "    recon_loss = tf.reduce_mean(mse_loss + ce_loss)\n",
    "\n",
    "    # Regularization (L2 norm of weights only)\n",
    "    l2_reg = tf.add_n([\n",
    "        tf.reduce_sum(tf.square(w))\n",
    "        for w in model.trainable_variables\n",
    "        if 'kernel' in w.name  # include weights only, not biases\n",
    "    ])\n",
    "    reg_term = (lam / 2.0) * l2_reg\n",
    "\n",
    "    return recon_loss + reg_term\n",
    "\n",
    "def train_autoencoder(x_train, x_val, learning_rate=1e-3, lam=1e-4, max_epochs=100, patience_limit=10,\n",
    "                      hidden_dims=[64, 32], loss_type='mse'):\n",
    "\n",
    "    # Model definition\n",
    "    input_dim = x.shape[1]\n",
    "    encoder, decoder = build_autoencoder(input_dim, hidden_dims)\n",
    "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # Shuffle training data\n",
    "        idx = np.random.permutation(len(x_train))\n",
    "        x_train_shuffled = x_train[idx]\n",
    "\n",
    "        # Per-instance gradient update\n",
    "        for xi in x_train_shuffled:\n",
    "            xi = tf.convert_to_tensor([xi], dtype=tf.float32)\n",
    "            with tf.GradientTape() as tape:\n",
    "                x_hat = autoencoder(xi, training=True)\n",
    "                loss = compute_loss(xi, x_hat, autoencoder, lam, loss_type)\n",
    "\n",
    "            gradients = tape.gradient(loss, autoencoder.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables))\n",
    "\n",
    "        # Validation loss\n",
    "        x_val_tensor = tf.convert_to_tensor(x_val, dtype=tf.float32)\n",
    "        x_val_hat = autoencoder(x_val_tensor, training=False)\n",
    "        val_loss = compute_loss(x_val_tensor, x_val_hat, autoencoder, lam, loss_type).numpy()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.5f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "            best_weights = autoencoder.get_weights()\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= patience_limit:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    autoencoder.set_weights(best_weights)\n",
    "    return encoder, decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
