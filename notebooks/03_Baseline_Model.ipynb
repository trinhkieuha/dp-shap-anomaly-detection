{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory to the parent directory\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "# Import relevant packages\n",
    "from src.eda import data_info\n",
    "from src.models import AutoencoderTrainer, AnomalyDetector, HybridLoss, AutoencoderTuner\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# Config\n",
    "pd.set_option('display.max_columns', None) # Ensure all columns are displayed\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read relevant files\n",
    "X_train = pd.read_feather(\"../data/processed/X_train.feather\")\n",
    "X_train_validate = pd.read_feather(\"../data/processed/X_train_validate.feather\")\n",
    "\n",
    "# Get data info\n",
    "var_info = data_info(X_train)\n",
    "all_cols = X_train.columns\n",
    "real_cols = var_info[var_info[\"var_type\"]==\"numerical\"][\"var_name\"].tolist()\n",
    "binary_cols = var_info[var_info[\"var_type\"]==\"binary\"][\"var_name\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 1s 1ms/step - loss: 19.6667 - val_loss: 15.6929\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 0s 811us/step - loss: 14.7092 - val_loss: 12.3819\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 0s 879us/step - loss: 11.9970 - val_loss: 9.8750\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 0s 855us/step - loss: 10.0057 - val_loss: 8.0447\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 0s 812us/step - loss: 8.5553 - val_loss: 6.6948\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 0s 819us/step - loss: 7.5209 - val_loss: 5.7005\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 0s 949us/step - loss: 6.6994 - val_loss: 4.9521\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 0s 855us/step - loss: 6.0981 - val_loss: 4.3800\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 0s 802us/step - loss: 5.6260 - val_loss: 3.9349\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 0s 810us/step - loss: 5.2347 - val_loss: 3.5758\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 0s 812us/step - loss: 4.8648 - val_loss: 3.2718\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 0s 804us/step - loss: 4.5941 - val_loss: 3.0207\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 0s 807us/step - loss: 4.3875 - val_loss: 2.8089\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 0s 810us/step - loss: 4.1757 - val_loss: 2.6258\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 0s 805us/step - loss: 3.9855 - val_loss: 2.4651\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 0s 805us/step - loss: 3.8379 - val_loss: 2.3215\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 0s 805us/step - loss: 3.6815 - val_loss: 2.1970\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 0s 808us/step - loss: 3.5642 - val_loss: 2.0832\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 0s 806us/step - loss: 3.4401 - val_loss: 1.9826\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 0s 806us/step - loss: 3.3363 - val_loss: 1.8872\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 0s 805us/step - loss: 3.2429 - val_loss: 1.8056\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 0s 807us/step - loss: 3.1732 - val_loss: 1.7293\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 0s 867us/step - loss: 3.0709 - val_loss: 1.6633\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 0s 851us/step - loss: 2.9929 - val_loss: 1.5955\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 0s 811us/step - loss: 2.9209 - val_loss: 1.5372\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 0s 816us/step - loss: 2.8675 - val_loss: 1.4811\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 0s 885us/step - loss: 2.8055 - val_loss: 1.4312\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 0s 910us/step - loss: 2.7645 - val_loss: 1.3847\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 0s 797us/step - loss: 2.6947 - val_loss: 1.3357\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 0s 789us/step - loss: 2.6699 - val_loss: 1.2977\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 0s 899us/step - loss: 2.5907 - val_loss: 1.2562\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 0s 815us/step - loss: 2.5378 - val_loss: 1.2179\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 0s 791us/step - loss: 2.5430 - val_loss: 1.1825\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 0s 770us/step - loss: 2.4953 - val_loss: 1.1498\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 0s 794us/step - loss: 2.4375 - val_loss: 1.1199\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 0s 851us/step - loss: 2.4115 - val_loss: 1.0901\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 0s 803us/step - loss: 2.3486 - val_loss: 1.0603\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 0s 804us/step - loss: 2.3327 - val_loss: 1.0334\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 0s 827us/step - loss: 2.3166 - val_loss: 1.0079\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 0s 816us/step - loss: 2.2869 - val_loss: 0.9858\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 0s 820us/step - loss: 2.2506 - val_loss: 0.9600\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 0s 898us/step - loss: 2.2265 - val_loss: 0.9385\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 2.1869 - val_loss: 0.9163\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 0s 906us/step - loss: 2.1416 - val_loss: 0.8941\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 0s 945us/step - loss: 2.1771 - val_loss: 0.8797\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 0s 852us/step - loss: 2.1165 - val_loss: 0.8579\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 0s 832us/step - loss: 2.1262 - val_loss: 0.8398\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 0s 884us/step - loss: 2.0795 - val_loss: 0.8223\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 0s 828us/step - loss: 2.0762 - val_loss: 0.8049\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 0s 860us/step - loss: 2.0150 - val_loss: 0.7885\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 0s 908us/step - loss: 2.0065 - val_loss: 0.7735\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 0s 885us/step - loss: 2.0029 - val_loss: 0.7576\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 0s 829us/step - loss: 1.9873 - val_loss: 0.7444\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 0s 835us/step - loss: 1.9542 - val_loss: 0.7308\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 0s 847us/step - loss: 1.9672 - val_loss: 0.7195\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 0s 820us/step - loss: 1.9418 - val_loss: 0.7056\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 0s 822us/step - loss: 1.9232 - val_loss: 0.6938\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 0s 827us/step - loss: 1.9448 - val_loss: 0.6843\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 0s 819us/step - loss: 1.9008 - val_loss: 0.6731\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 0s 825us/step - loss: 1.8898 - val_loss: 0.6596\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 0s 828us/step - loss: 1.8684 - val_loss: 0.6487\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 0s 824us/step - loss: 1.8372 - val_loss: 0.6386\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 0s 815us/step - loss: 1.8391 - val_loss: 0.6280\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 0s 821us/step - loss: 1.8231 - val_loss: 0.6221\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.8052 - val_loss: 0.6131\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 0s 823us/step - loss: 1.8028 - val_loss: 0.6028\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 0s 823us/step - loss: 1.7772 - val_loss: 0.5933\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 0s 824us/step - loss: 1.8006 - val_loss: 0.5867\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 0s 867us/step - loss: 1.7615 - val_loss: 0.5797\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 0s 822us/step - loss: 1.7681 - val_loss: 0.5713\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 0s 819us/step - loss: 1.7515 - val_loss: 0.5637\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 0s 827us/step - loss: 1.7466 - val_loss: 0.5580\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 0s 824us/step - loss: 1.7190 - val_loss: 0.5507\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 0s 819us/step - loss: 1.7007 - val_loss: 0.5440\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 0s 829us/step - loss: 1.6891 - val_loss: 0.5358\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 0s 830us/step - loss: 1.6960 - val_loss: 0.5295\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 0s 828us/step - loss: 1.6795 - val_loss: 0.5243\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 0s 826us/step - loss: 1.6824 - val_loss: 0.5189\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 0s 825us/step - loss: 1.6706 - val_loss: 0.5140\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 0s 832us/step - loss: 1.6483 - val_loss: 0.5065\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 0s 829us/step - loss: 1.6597 - val_loss: 0.5027\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 0s 821us/step - loss: 1.6498 - val_loss: 0.4978\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 0s 820us/step - loss: 1.6389 - val_loss: 0.4928\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 0s 826us/step - loss: 1.6261 - val_loss: 0.4899\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 0s 824us/step - loss: 1.5972 - val_loss: 0.4827\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 0s 819us/step - loss: 1.5906 - val_loss: 0.4777\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 0s 819us/step - loss: 1.5876 - val_loss: 0.4744\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 0s 826us/step - loss: 1.6011 - val_loss: 0.4688\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 0s 824us/step - loss: 1.5773 - val_loss: 0.4656\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 0s 833us/step - loss: 1.6193 - val_loss: 0.4619\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 0s 844us/step - loss: 1.5568 - val_loss: 0.4570\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 0s 861us/step - loss: 1.5690 - val_loss: 0.4542\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 0s 836us/step - loss: 1.5656 - val_loss: 0.4498\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 0s 785us/step - loss: 1.5504 - val_loss: 0.4462\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 0s 801us/step - loss: 1.5499 - val_loss: 0.4433\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 0s 826us/step - loss: 1.5520 - val_loss: 0.4397\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 0s 825us/step - loss: 1.5261 - val_loss: 0.4359\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 0s 825us/step - loss: 1.5335 - val_loss: 0.4318\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 0s 813us/step - loss: 1.5221 - val_loss: 0.4298\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 0s 824us/step - loss: 1.4949 - val_loss: 0.4254\n"
     ]
    }
   ],
   "source": [
    "ae = AutoencoderTrainer(\n",
    "    input_dim=X_train.shape[1],\n",
    "    real_cols=real_cols,\n",
    "    binary_cols=binary_cols,\n",
    "    all_cols=all_cols,\n",
    "    hidden_dims=[64],\n",
    "    learning_rate=1e-2,\n",
    "    dropout_rate=0.2\n",
    ")\n",
    "autoencoder = ae.train(X_train, X_train_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the anomaly detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7097844112769486, 'precision': 0.369431643625192, 'recall': 0.6260303687635574, 'f1_score': 0.4646594751247786, 'auc': 0.726876360488752}\n"
     ]
    }
   ],
   "source": [
    "# Read relevant files\n",
    "X_validate = pd.read_feather(\"../data/processed/X_validate.feather\")\n",
    "y_validate = pd.read_feather(\"../data/processed/y_validate.feather\")\n",
    "\n",
    "# After training\n",
    "detector = AnomalyDetector(\n",
    "    model=autoencoder,\n",
    "    real_cols=real_cols,\n",
    "    binary_cols=binary_cols,\n",
    "    all_cols=all_cols,\n",
    "    lam=ae.lam,\n",
    ")\n",
    "\n",
    "# Compute scores\n",
    "scores = detector.compute_anomaly_scores(X_validate)\n",
    "\n",
    "# Detect\n",
    "y_pred = detector.detect(scores, 0.3)\n",
    "\n",
    "# Evaluate\n",
    "metrics = detector.evaluate(y_pred, y_validate, scores)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with: {'hidden_dims': [64], 'learning_rate': 0.01, 'lam': 0.001}\n",
      "  Threshold 0.6370 → F1_score = 0.4894\n",
      "  Threshold 0.6900 → F1_score = 0.4970\n",
      "  Threshold 0.7819 → F1_score = 0.5007\n",
      "  Threshold 1.1300 → F1_score = 0.4513\n",
      "  Threshold 2.1073 → F1_score = 0.2752\n",
      "\n",
      "Training with: {'hidden_dims': [64, 32], 'learning_rate': 0.01, 'lam': 0.001}\n",
      "  Threshold 0.4441 → F1_score = 0.4751\n",
      "  Threshold 0.4940 → F1_score = 0.4815\n",
      "  Threshold 0.5921 → F1_score = 0.4534\n",
      "  Threshold 0.8437 → F1_score = 0.3996\n",
      "  Threshold 2.2144 → F1_score = 0.2446\n",
      "\n",
      "Best parameters found:\n",
      "- hidden_dims: [64]\n",
      "- learning_rate: 0.01\n",
      "- lam: 0.001\n",
      "- threshold: 0.7818549156188963\n",
      "Best validation F1_score: 0.5007\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'hidden_dims': [[64], [64, 32]],\n",
    "    'learning_rate': [1e-2, 1e-3],\n",
    "    'lam': [1e-3, 1e-4],\n",
    "    'dropout_rate': [None, 0.2],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'batch_size': [32, 64],\n",
    "}\n",
    "\n",
    "tuner = AutoencoderTuner(X_train, X_train_validate, X_validate, y_validate, real_cols, binary_cols, all_cols, verbose=False)\n",
    "best_model, best_params, best_auc, results_df = tuner.tune(param_grid, metric=\"f1_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_dims</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>lam</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.636967</td>\n",
       "      <td>0.744087</td>\n",
       "      <td>0.408787</td>\n",
       "      <td>0.609544</td>\n",
       "      <td>0.489377</td>\n",
       "      <td>0.740467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[64]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.689969</td>\n",
       "      <td>0.779349</td>\n",
       "      <td>0.459022</td>\n",
       "      <td>0.541866</td>\n",
       "      <td>0.497016</td>\n",
       "      <td>0.740467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[64]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.781855</td>\n",
       "      <td>0.812167</td>\n",
       "      <td>0.538155</td>\n",
       "      <td>0.468113</td>\n",
       "      <td>0.500696</td>\n",
       "      <td>0.740467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[64]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.130030</td>\n",
       "      <td>0.827878</td>\n",
       "      <td>0.629170</td>\n",
       "      <td>0.351844</td>\n",
       "      <td>0.451308</td>\n",
       "      <td>0.740467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[64]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.107344</td>\n",
       "      <td>0.817928</td>\n",
       "      <td>0.691099</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.275191</td>\n",
       "      <td>0.740467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.444139</td>\n",
       "      <td>0.736929</td>\n",
       "      <td>0.396858</td>\n",
       "      <td>0.591757</td>\n",
       "      <td>0.475096</td>\n",
       "      <td>0.722153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.493959</td>\n",
       "      <td>0.772541</td>\n",
       "      <td>0.444689</td>\n",
       "      <td>0.524946</td>\n",
       "      <td>0.481496</td>\n",
       "      <td>0.722153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.592141</td>\n",
       "      <td>0.794362</td>\n",
       "      <td>0.487282</td>\n",
       "      <td>0.423861</td>\n",
       "      <td>0.453364</td>\n",
       "      <td>0.722153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.843698</td>\n",
       "      <td>0.811644</td>\n",
       "      <td>0.557021</td>\n",
       "      <td>0.311497</td>\n",
       "      <td>0.399555</td>\n",
       "      <td>0.722153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.214415</td>\n",
       "      <td>0.810247</td>\n",
       "      <td>0.614311</td>\n",
       "      <td>0.152711</td>\n",
       "      <td>0.244614</td>\n",
       "      <td>0.722153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hidden_dims  learning_rate    lam  threshold  accuracy  precision    recall  \\\n",
       "0        [64]           0.01  0.001   0.636967  0.744087   0.408787  0.609544   \n",
       "1        [64]           0.01  0.001   0.689969  0.779349   0.459022  0.541866   \n",
       "2        [64]           0.01  0.001   0.781855  0.812167   0.538155  0.468113   \n",
       "3        [64]           0.01  0.001   1.130030  0.827878   0.629170  0.351844   \n",
       "4        [64]           0.01  0.001   2.107344  0.817928   0.691099  0.171800   \n",
       "5    [64, 32]           0.01  0.001   0.444139  0.736929   0.396858  0.591757   \n",
       "6    [64, 32]           0.01  0.001   0.493959  0.772541   0.444689  0.524946   \n",
       "7    [64, 32]           0.01  0.001   0.592141  0.794362   0.487282  0.423861   \n",
       "8    [64, 32]           0.01  0.001   0.843698  0.811644   0.557021  0.311497   \n",
       "9    [64, 32]           0.01  0.001   2.214415  0.810247   0.614311  0.152711   \n",
       "\n",
       "   f1_score       auc  \n",
       "0  0.489377  0.740467  \n",
       "1  0.497016  0.740467  \n",
       "2  0.500696  0.740467  \n",
       "3  0.451308  0.740467  \n",
       "4  0.275191  0.740467  \n",
       "5  0.475096  0.722153  \n",
       "6  0.481496  0.722153  \n",
       "7  0.453364  0.722153  \n",
       "8  0.399555  0.722153  \n",
       "9  0.244614  0.722153  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8253469494632103, 'precision': 0.6333865814696485, 'recall': 0.33961456102783727, 'f1_score': 0.44215221633677165, 'auc': 0.7433049826125699}\n"
     ]
    }
   ],
   "source": [
    "# Read relevant files\n",
    "X_test = pd.read_feather(\"../data/processed/X_test.feather\")\n",
    "y_test = pd.read_feather(\"../data/processed/y_test.feather\")\n",
    "\n",
    "# After training\n",
    "detector = AnomalyDetector(\n",
    "    model=best_model,\n",
    "    real_cols=real_cols,\n",
    "    binary_cols=binary_cols,\n",
    "    all_cols=all_cols,\n",
    "    lam=ae.lam,\n",
    ")\n",
    "\n",
    "# Compute scores\n",
    "scores = detector.compute_anomaly_scores(X_test)\n",
    "\n",
    "# Detect\n",
    "y_pred = detector.detect(scores, best_params['threshold'])\n",
    "\n",
    "# Evaluate\n",
    "metrics = detector.evaluate(y_pred, y_test, scores)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
