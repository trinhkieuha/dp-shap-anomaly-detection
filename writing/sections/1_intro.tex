% Introduce DP
Artificial intelligence (AI) has rapidly evolved and become deeply integrated into modern life, especially with the emergence and advancement of multimodal models, such as GPT-4, Gemini, and Claude 3, which demonstrate advanced capabilities in processing text, images, and audio. Those models, however, rely on vast amounts of extracted online data, often inadvertently collecting personally identifiable information without the owners’ awareness, thereby raising significant privacy concerns \citep{perrault2024artificial}. In response to these privacy issues, researchers have introduced privacy-preserving frameworks and criteria to mitigate data exposure risks while maintaining AI performance, including \textit{$\varepsilon$-differential privacy} (DP) introduced by \cite{dwork2006calibrating}, where $\varepsilon$ stands for the privacy budget parameter that quantifies the allowed privacy loss. This framework provides a formal mathematical guarantee of privacy by ensuring that the inclusion or exclusion of any individual has a negligible impact on the overall dataset, thereby implying its resistance to re-identification attacks. In data analysis, DP supports privacy protection by calibrating noise to the sensitivity of the function, enabling useful data insights while maintaining strong privacy protections \citep{dwork2006calibrating}. Therefore, DP is widely adopted across various data analysis domains such as machine learning \citep{abadi2016deep, papernot2016semi}, statistical analysis \citep{wasserman2010statistical, steinberger2024efficiency}, and database queries \citep{mcsherry2009privacy}.

% Additional points:
% \begin{itemize}
%     \item Regulatory frameworks of data protection have been adopted.
%     \item Regulatory Compliance – DP aligns with privacy laws such as GDPR, HIPAA, and CCPA, making it a preferred method for handling sensitive AI data.
% \end{itemize}

% Challenges of adopting DP into DL
While DP effectively protects privacy, applying it to \textit{deep learning} (DL) models introduces unique challenges. DL is a subset of AI that leverages multi-layered artificial neural networks to automatically learn hierarchical representations of data, enabling it to extract intricate patterns and solve complex problems \citep{noor2024survey}. In recent years, advancements in neural networks techniques brought notable achievements which make them indispensable for applications in privacy-sensitive fields such as healthcare \citep{shamshirband2021review}, biometrics recognition \citep{minaee2023biometrics}, cybersecurity \citep{dixit2021deep}, etc. \textit{Differentially private stochastic gradient descent} (DP-SGD) integrates DP into DL training, particularly by clipping gradients and adding calibrated noise, ensuring privacy while retaining learning capabilities. Like other DP mechanisms, in DP-SGD, injected DP noises affect model convergence with reduced accuracy and stability, and this trade-off is controlled by the $\epsilon$ parameter \citep{abadi2016deep}.

% Need for XAI in DL 
In addition to challenges regarding privacy, DL models are highly complex and opaque with multi-layers, making it difficult to understand their algorithmic decisions. This black-box nature is especially problematic for high-stakes applications as it obstructs the understanding and justification of the decision-making process \citep{noor2024survey}. As a result, \textit{eXplainable AI} (XAI) techniques become crucial for model interpretability and explainability. XAI methods, such as Shapley additive explanations (SHAP), locally-interpretable model-agnostic explanations (LIME), and counterfactual
multi-granularity graph supporting fact extraction (CMGE), generates interpretable explanations that enable comprehending and trusting the decisions made by DL models \citep{yang2023survey}. 

% DP and XAI: opposing goals
Both supporting responsible AI, DP, and XAI apparently have opposing goals, which is likely to result in the trade-off between privacy and explainability \citep{ezzeddine2024differential}. On one hand, XAI methods seek to identify key factors influencing model predictions, which may inadvertently reveal sensitive personal information through explanations derived from model parameters \citep{patel2022model}. On the other hand, DP incorporates calibrated noise to enforce privacy guarantees, which distorts feature attributions and reduces the reliability of model explanations \citep{bozorgpanah2022privacy}. This inherent conflict between privacy and explainability underscores the need for a deeper investigation into their trade-offs, particularly in high-stakes applications where both transparency and data protection are critical. Understanding how DP affects the effectiveness of XAI methods plays a pivotal role in designing models that strike a balance between preserving privacy and maintaining interpretability.

% AD needs both DP and XAI
\textit{Anomaly detection} (AD), the process of spotting data points that noticeably differ from the norm, is one of the common practices in high-stakes domains where handling sensitive data poses major privacy concerns \cite{fan2013differentially}. AD often suffers from class imbalance, where the rarity of anomalies leads to a low recall rate, resulting in missed critical anomalies \citep{pang2021deep}. Studying why a data point is classified as anomaly not only enhances trust among stakeholders but also helps determine the reasons for model failure to improve it. Given the critical need for both privacy and explainability in AD systems, quantifying the trade-off between DP and XAI in deep AD models is both scientifically valuable and practically beneficial. This type of study lays the foundation for developing an effective AD system that balances privacy guarantees with trustworthiness, contributing to the broader goal of responsible AI.

% Need more studies on the trade-offs in AD
DP has emerged as a key method for preserving data confidentiality in deep AD applications, including backdoor attack detection \citep{du2019robust}, epidemic outbreak detection \citep{fan2013differentially}, and fraud detection \citep{wang2018privacy}. Existing studies in these areas primarily focus on maintaining model utility and predictive accuracy while incorporating calibrated noise to enhance privacy protection. However, they often overlook the impact of DP on interpretability. The intersection of XAI and DP in deep AD remains largely unexplored, with limited empirical research examining how privacy-preserving noise affects the quality of model explanations. Most studies assessing DP’s effects primarily focus on utility degradation, assuming a general trade-off between privacy and performance \citep{wu2020value, arous2023exploring}, without extending this analysis to interpretability metrics. While some research investigates the impact of DP on explainability in machine learning models \citep{naidu2021differential, bozorgpanah2022privacy}, these studies do not specifically address AD, where feature attribution methods may behave differently due to the rarity and complexity of anomalies. This study aims to bridge these gaps by systematically evaluating the influence of DP on XAI techniques in deep AD, providing empirical insights into the privacy-interpretability trade-off across varying privacy budgets ($\varepsilon$).

% Objective of this study
This study systematically examines how DP mechanisms, including DP-SGD and post-hoc DP, impact model explainability in deep AD. While \cite{saifullah2022privacy} broadly explores DP’s effects on XAI, we specifically investigate SHAP-based interpretability in AD models under DP budgets ($\varepsilon$). Unlike \cite{ezzeddine2024differential}, which focuses on post-hoc DP explainability in standard machine learning models, our work extends this analysis to DL-based AD, with noise incorporated directly into the training process. This setting introduces greater interpretability challenges due to the architectural complexity of neural networks and the compounding effects of injected noise. By bridging DP, XAI, and AD, this study provides empirical insights into the privacy-explainability trade-off in DL models, offering a nuanced understanding of how privacy-preserving noise affects model interpretability in AD contexts.

% Methodology of this study
In terms of methodology, this study evaluates the impact of DP-SGD and post-hoc DP on DL-based AD models, particularly focusing on how privacy-preserving mechanisms influence both performance and interpretability. To ensure practical relevance, we utilize real-world AD datasets, implement autoencoder architectures, and apply KernelSHAP to measure both predictive accuracy and explainability before and after DP noise injection. Model evaluation is conducted across multiple dimensions: performance metrics include the area under the receiver operating characteristic curve (AUC-ROC), accuracy, precision, recall, and F1-score, while explainability metrics assess SHAP Stability, Fidelity, Sensitivity, and Infidelity. Additionally, we analyze the privacy-interpretability trade-off by examining how DP budgets (\(\varepsilon = 0.01, 0.1, 1, 5\)) affect both detection accuracy and model interpretability, providing a comprehensive understanding of DP’s impact on deep AD models.

\paragraph{Findings} What are the main findings that come out of your research?

\paragraph{Comparison to Literature} Briefly how your research compares to existing lines of literature (or possibly alternatively refer forward to a literature review section).

% Structure
This paper is structured as follows. Section~\ref{s:lit_rev} (Literature Review) explores prior research on DP, XAI, and AD, identifying key gaps in the intersection of these fields. Section~\ref{s:background} (Background) provides theoretical foundations on DP, autoencoders, and XAI. Section~\ref{s:method} (Methodology) details the datasets, model architectures, DP mechanisms, and XAI techniques used in our study. Section~\ref{s:emp} (Experimental Results) analyzes the impact of DP on the privacy-utility-explainability trade-off in AD models. Section~\ref{s:discuss} (Discussion) presents an interpretation of the results and critically evaluates the limitations of the study.. Finally, Section~\ref{s:concl} (Conclusion) summarizes key findings, highlights contributions, and outlines directions for future research in DP-XAI for AD.