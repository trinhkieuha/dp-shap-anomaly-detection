\textit{Anomaly detection} (AD), the process of spotting data points that noticeably differ from the norm, is one of the common practices in high-stakes domains, such as fraud, cyber intrusions, or equipment failures, where handling sensitive data poses major privacy concerns \citep{fan2013differentially}. To safeguard individual privacy, \textit{differential privacy} (DP) provides a mathematically rigorous framework for bounding information leakage, while \textit{eXplainable AI} (XAI) methods offer interpretable insights into model decisions. However, integrating DP noise during deep learning (DL) training may distort feature attributions, complicating the explanation of why an anomaly was flagged.

Bridging these concerns, our research asks: \emph{How does DP affect the interpretability of DL-based AD models under varying privacy budgets?} By quantifying the trade-off between privacy guarantees and explainability metrics, this work aims to inform the design of AD systems that uphold both data confidentiality and decision transparency.

% Introduce DP
The need for privacy-preserving analytics has grown alongside advances in artificial intelligence (AI). Multimodal models like GPT-4, Gemini, and Claude 3 showcase AI’s capacity to process text, images, and audio at scale, yet these systems often train on vast online datasets that inadvertently collect personally identifiable information without consent \citep{perrault2024artificial}. DP, formalized by \citet{dwork2006calibrating}, addresses this by adding calibrated noise, controlled by the privacy budget~$\varepsilon$, to ensure that any one individual’s data negligibly affects overall analyses. Through DP’s noise addition mechanisms, analysts can extract useful insights while providing provable resistance to re‑identification, making it prevalent in machine learning \citep{abadi2016deep,papernot2016semi}, statistical inference \citep{wasserman2010statistical,steinberger2024efficiency}, and database querying \citep{mcsherry2009privacy}.

% Challenges of adopting DP into DL
While DP effectively protects privacy, applying it to DL models introduces unique challenges. In recent years, advancements in DL techniques brought notable achievements which make them indispensable for applications in privacy-sensitive fields such as healthcare \citep{shamshirband2021review}, biometrics recognition \citep{minaee2023biometrics}, cybersecurity \citep{dixit2021deep}, etc. \textit{Differentially private stochastic gradient descent} (DP-SGD) integrates DP into DL training, particularly by clipping gradients and adding calibrated noise, ensuring privacy while retaining learning capabilities. Like other DP mechanisms, in DP-SGD, injected DP noises affect model convergence with reduced accuracy and stability, and this trade-off is controlled by the $\epsilon$ parameter \citep{abadi2016deep}.

% Need for XAI in DL 
In addition to challenges regarding privacy, DL models are highly complex and opaque with multi-layers, making it difficult to understand their algorithmic decisions. This black-box nature is especially problematic for high-stakes applications as it obstructs the understanding and justification of the decision-making process \citep{noor2024survey}. As a result, XAI techniques become crucial for model interpretability and explainability. XAI methods, such as Shapley additive explanations (SHAP), locally-interpretable model-agnostic explanations (LIME), and counterfactual
multi-granularity graph supporting fact extraction (CMGE), generates interpretable explanations that enable comprehending and trusting the decisions made by DL models \citep{yang2023survey}. 

% DP and XAI: opposing goals
DP and XAI, though both supporting responsible AI, apparently have opposing goals, which result in the trade-off between privacy and explainability \citep{ezzeddine2024differential}. XAI seeks to reveal influential factors, potentially exposing sensitive information through explanations derived from model parameters \citep{patel2022model}, while DP perturbations tend to distort feature attributions and reduce the reliability of model explanations \citep{bozorgpanah2022privacy}. This inherent conflict between privacy and explainability necessitates rigorous study, particularly in high‑stakes settings where both guarantees are imperative.

These opposing forces, privacy protection and interpretability, converge in the context of AD, which itself contends with severe class imbalance and low recall for rare anomalies \citep{pang2021deep}. Studying why a data point is classified as anomaly not only enhances trust among stakeholders but also helps determine the reasons for model failure to improve it. Given the critical need for both privacy and explainability in AD systems, quantifying the trade-off between DP and XAI in deep AD models is both scientifically valuable and practically beneficial. This type of study lays the foundation for developing an effective AD system that balances privacy guarantees with trustworthiness, contributing to the broader goal of responsible AI.

% Need more studies on the trade-offs in AD
Although DP has been applied in deep AD for tasks like backdoor detection \citep{du2019robust}, epidemic monitoring \citep{fan2013differentially}, and fraud detection \citep{wang2018privacy}, the impact of DP on XAI within AD remains underexplored. Existing work focuses on utility loss under DP \citep{wu2020value,arous2023exploring} without assessing interpretability, and studies on DP’s effect on explainability have yet to address the unique demands of anomaly attribution \citep{naidu2021differential,bozorgpanah2022privacy}.

% Objective of this study
This thesis aims to bridge these gaps by systematically evaluating the influence of DP mechanisms on XAI techniques in deep AD, providing empirical insights into the privacy-interpretability trade-off across varying privacy budgets ($\varepsilon$). While \cite{saifullah2022privacy} broadly explores DP’s effects on XAI, we specifically investigate SHAP-based interpretability in AD models under DP budgets ($\varepsilon$). Unlike \cite{ezzeddine2024differential}, which focuses on post-hoc DP explainability in standard machine learning models, our work extends this analysis to DL-based AD, with noise incorporated directly into the training process. This setting introduces greater interpretability challenges due to the architectural complexity of neural networks and the compounding effects of injected noise. By bridging DP, XAI, and AD, this study provides empirical insights into the privacy-explainability trade-off in DL models, offering a nuanced understanding of how privacy-preserving noise affects model interpretability in AD contexts.

% Methodology of this study
In terms of methodology, this study assesses the impact of DP-SGD and output perturbation on DL-based AD models, particularly focusing on how privacy-preserving mechanisms with various privacy budgets influence both performance and interpretability. To ensure practical relevance, we utilize real-world AD datasets, implement autoencoder architectures, and apply KernelSHAP to measure both predictive accuracy and explainability before and after DP noise injection. Model evaluation is conducted across multiple dimensions: performance metrics include the area under the receiver operating characteristic curve (AUC-ROC), precision, recall, and F1-score, while explainability metrics assess SHAP Stability, Fidelity, Sensitivity, and Infidelity. Additionally, we analyze the privacy-interpretability trade-off by examining how DP budgets affect both detection accuracy and model interpretability, providing a comprehensive understanding of DP’s impact on deep AD models.

\paragraph{Findings} What are the main findings that come out of your research?

\paragraph{Comparison to Literature} Briefly how your research compares to existing lines of literature (or possibly alternatively refer forward to a literature review section).

% Structure
This paper is structured as follows. Section~\ref{s:lit_rev} (Literature Review) explores prior research on DP, XAI, and AD, identifying key gaps in the intersection of these fields. Section~\ref{s:background} (Background) provides theoretical foundations on DP, autoencoders, and XAI. Section~\ref{s:method} (Methodology) details the datasets, model architectures, DP mechanisms, and XAI techniques used in our study. Section~\ref{s:emp} (Experimental Results) analyzes the impact of DP on the privacy-utility-explainability trade-off in AD models. Section~\ref{s:discuss} (Discussion) presents an interpretation of the results and critically evaluates the limitations of the study.. Finally, Section~\ref{s:concl} (Conclusion) summarizes key findings, highlights contributions, and outlines directions for future research in DP-XAI for AD.