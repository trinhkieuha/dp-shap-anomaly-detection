\subsection{Differential Privacy} \label{s:background_dp}

DP is a formal framework that quantifies privacy guarantees for individuals in a dataset. It generally ensures that for any two datasets differing by one individual, the output is very unlikely to be much more or less probable depending on which dataset is used \citep{dwork2014algorithmic}.

\begin{definition}[$(\varepsilon, \delta)$-DP {\citep[Definition 2.4, pg.~17-18]{dwork2014algorithmic}}]
A randomized algorithm $\mathcal{M}$ satisfies $(\varepsilon, \delta)$-DP if, for all pairs of datasets $x$ and $y$ that differ in the data of only one individual, and for all measurable subsets $S$ of the output space, the following holds:
\[
\Pr[\mathcal{M}(x) \in S] \leq e^{\varepsilon} \cdot \Pr[\mathcal{M}(y) \in S] + \delta,
\]
where $\varepsilon$, often known as privacy budget, controls the multiplicative privacy loss, while $\delta$ (typically smaller than $1/|x|$) allows for a small additive probability of failure in the privacy guarantee.
\end{definition}

In the context of AD, $x$ refers to the full dataset containing monitoring data used to train or evaluate an AD model. On the other hand, $y$ represents a modified version of $x$ in which the data of one individual entity has been either removed or replaced. The DP guarantee ensures that the output of the AD system, such as whether a data point is flagged as anomalous or the shape of the learned normal distribution, will not significantly change depending on whether that individual’s data is included. As a result, DP provides a robust foundation for building privacy-preserving anomaly detectors that are resilient against linkage attacks, even when adversaries possess auxiliary knowledge \citep[pg.~22]{dwork2014algorithmic}.

A common concept to formally quantify how much the output of a model or query can change due to the presence or absence of a single individual’s data is sensitivity, which measures the maximum difference in a function’s output across neighboring datasets.

\begin{definition}[$\ell_p$-sensitivity {\citep[Definition 3.1 and 3.8]{dwork2014algorithmic}}]\label{def:sensitivity}
Let $\mathcal{X}$ be the set of all possible values that an individual’s data can assume. The extent to which a single individual’s data can change the function $f : \mathbb{N}^{|\mathcal{X}|} \rightarrow \mathbb{R}^k$, called $\ell_p$-sensitivity ($p \geq 1$), is formally given by:
\[
\Delta_p f = \max \|f(x) - f(y)\|_p,
\]
where $x$ and $y$ are any two neighboring datasets in $\mathbb{N}^{|\mathcal{X}|}$ that differ in only one individual’s data.
\end{definition}

To limit the impact quantified by sensitivity and ensure DP, randomized mechanisms are applied to the output of the function. Two of the most widely used mechanisms are Laplace mechanism (Definition~\ref{def:laplace}) and Gaussian mechanism (Definition~\ref{def:gaussian}).

\begin{definition}[Laplace Mechanism {\citep[Definition 3.3]{dwork2014algorithmic}}]\label{def:laplace}
Let $f : \mathbb{N}^{|\mathcal{X}|} \rightarrow \mathbb{R}^k$ be a function with global $\ell_1$-sensitivity $\Delta f$. The \emph{Laplace mechanism} adds independent noise to each component of $f(x)$ drawn from the Laplace distribution with scale parameter $\Delta f / \varepsilon$, and is defined as:
\[
\mathcal{M}_L(x, f, \varepsilon) = f(x) + (Y_1, \ldots, Y_k),
\]
where each $Y_i \overset{\text{i.i.d.}}{\sim} \mathrm{Lap}(\Delta f / \varepsilon)$ independently.
\end{definition}

\begin{theorem}[Privacy Guarantee of Laplace Mechanism {\citep[Theorem 3.6]{dwork2014algorithmic}}]\label{the:laplace_guarantee}
The Laplace mechanism is $(\varepsilon,0)$-DP.
\end{theorem}

\begin{theorem}[Accuracy Bound of Laplace Mechanism {\citep[Theorem 3.8]{dwork2014algorithmic}}]\label{the:laplace_accuracy}
For any $\delta \in (0, 1]$, with probability at least $1 - \delta$, the output $y = \mathcal{M}_L(x, f, \varepsilon)$ satisfies:
\[
\|f(x) - y\|_\infty \leq \ln\left(\frac{k}{\delta}\right) \cdot \frac{\Delta f}{\varepsilon}.
\]
\end{theorem}

The Laplace mechanism guarantees pure $(\varepsilon,0)$-DP (Theorem~\ref{the:laplace_guarantee}) while bounding the error introduced by the added noise (Theorem~\ref{the:laplace_accuracy}). The scale of the noise added is proportional to $\Delta f / \varepsilon$, where $\Delta f$ is the sensitivity of the query function as defined in Definition~\ref{def:sensitivity}. Therefore, a smaller $\varepsilon$ increases the scale of the noise, making the output more private but less precise. Conversely, a larger $\varepsilon$ reduces the noise, allowing for more accurate results but offering weaker privacy protection. Thus, $\varepsilon$ directly determines the noise scale and serves as a key parameter in managing the privacy-utility trade-off.

\begin{definition}[Gaussian Mechanism {\citep[pg. 261]{dwork2014algorithmic}}]\label{def:gaussian}
Let $f : \mathbb{N}^{|\mathcal{X}|} \rightarrow \mathbb{R}^k$ be a function with global $\ell_2$-sensitivity $\Delta_2 f$. The \emph{Gaussian mechanism with parameter $\sigma$} adds independent Gaussian noise to each component of $f(x)$ and is defined as:
\[
\mathcal{M}_G(x, f, \sigma) = f(x) + (Y_1, \ldots, Y_k),
\]
where each $Y_i \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)$.
\end{definition}
\begin{theorem}[Privacy Guarantee of Gaussian Mechanism {\citep[Theorem A.1]{dwork2014algorithmic}}]\label{the:gaussian_guarantee}
For any $\epsilon \in (0, 1)$, if $c^2 > 2 \ln(1.25/\delta)$ and $\sigma \geq \frac{c \Delta_2 f}{\varepsilon}$, then the Gaussian mechanism is $(\varepsilon, \delta)$-DP.
\end{theorem}

Similar to the Laplace mechanism, the parameter $\varepsilon$ plays a central role in the privacy-utility trade-off of the Gaussian mechanism. According to the theorem, achieving a lower $\varepsilon$ corresponds to stronger privacy but requires adding more noise to the output, as the standard deviation $\sigma$ must scale at least proportionally to $1/\varepsilon$. This increased noise reduces the accuracy of the output, thereby decreasing utility.

Although both mechanisms use $\varepsilon$ to control the privacy-utility trade-off, they differ in the nature of the privacy guarantees and the resulting privacy-utility trade-offs. The Laplace mechanism ensures pure $(\varepsilon, 0)$-DP, while the Gaussian mechanism provides approximate $(\varepsilon, \delta)$-DP, allowing a small additive term $\delta$ to account for rare deviations where the pure $(\varepsilon, 0)$-DP does not hold. Laplace relies on $\ell_1$-sensitivity, whereas Gaussian uses $\ell_2$-sensitivity, which is often smaller in high-dimensional settings. As a result, the Gaussian mechanism typically requires less noise and offers better utility, though with slightly weaker privacy. Its thinner-tailed noise distribution also yields more stable outputs, making it attractive for tasks like AD where utility is critical.

\subsection{Differentially Private Stochastic Gradient Descent}

The most intuitive and straightforward approach to integrating DP into DL models is to apply DP mechanisms directly to the outputs of a black-box process. For instance, \cite{lu2022differentially} add calibrated noise to a selected output neuron at inference time to ensure privacy without altering the training procedure.

In contrast, most modern approaches integrate privacy into the training process itself. Stochastic gradient descent (SGD) is a core optimization method in machine learning, originally proposed by \citet{robbins1951stochastic} as a stochastic approximation technique. It updates model parameters using noisy gradient estimates based on individual training examples and has become fundamental in training neural networks. Its practical effectiveness in DL was notably demonstrated by \citet{lecun1998gradient} in the context of convolutional neural networks. A common variant, mini-batch SGD, accelerates training and stabilizes convergence by computing gradients over small random subsets of data.

While effective in practice, ordinary SGD lacks built-in privacy protection, making it vulnerable to privacy leakage when trained on sensitive data. Another widely adapted approach is proposed by \citet{abadi2016deep}, who introduce DP-SGD, a privacy-preserving extension of SGD that adds noise to the average of per-example clipped gradients, thereby bounding the influence of any single training example during optimization.

Key modifications of DP-SGD involve computing gradients $\mathbf{g}_t(\mathbf{x}^{(i)}) = \nabla \mathcal{L}_t(\mathbf{x}^{(i)})$ on a per-example basis within each lot $L_t$, clipping their $\ell_2$ norms to a fixed threshold $C$ via:
\begin{align} \label{eq:grad_clip}
\bar{\mathbf{g}}_t(\mathbf{x}^{(i)}) \leftarrow \mathbf{g}_t(\mathbf{x}^{(i)}) / \max(1, \|\mathbf{g}_t(\mathbf{x}^{(i)})\|_2 / C),
\end{align}
and then adding Gaussian noise $\mathcal{N}(0, \sigma^2 C^2 \mathbf{I})$ to the average of the clipped gradients. A smaller threshold \(C\) enhances privacy by reducing sensitivity, but overly aggressive clipping may distort gradient directions and hinder convergence, thus impairing model utility. 

The resulting noisy gradient $\tilde{\mathbf{g}}_t$ is then used to update the model parameters. This process bounds the sensitivity of each update step, thereby enabling formal $(\varepsilon, \delta)$-DP guarantees for each update step as long as \(\sigma \geq \sqrt{2 \ln \left( 1.25/\delta \right)}/\varepsilon\) according to Theorem~\ref{the:gaussian_guarantee}.

Accurately tracking the cumulative privacy loss of DP-SGD over many training steps is essential, as each iteration involves applying a DP mechanism. Traditional composition theorems, such as the basic and advanced composition results from \citet{dwork2014algorithmic}, provide formal guarantees but tend to yield overly conservative bounds in DL settings due to the large number of iterations. To address this, \citet{abadi2016deep} introduced the Moments Accountant, a privacy accounting method that tracks higher-order moments of the privacy loss random variable and offers tighter bounds than classical composition. However, it tracks privacy loss by evaluating the cumulant generating function (CGF) at a fixed list of moment orders, which requires predefining those orders and can limit flexibility or lead to suboptimal accuracy \citep{wang2019subsampled}.

To overcome these limitations and enable precise calibration of privacy-preserving mechanisms, this study adopts Rényi differential privacy (RDP) \citep{mironov2017renyi}, which generalizes classical DP and supports tight composition across diverse randomized mechanisms.

\begin{definition}[$(\alpha, \varepsilon)$-RDP {\citep[Definition 3 \& 4]{mironov2017renyi}}]
A randomized mechanism $\mathcal{M} : \mathcal{D} \to \mathcal{R}$ is said to satisfy $(\alpha, \varepsilon)$-Rényi differential privacy (RDP) if, for all pairs of adjacent datasets $x, y \in \mathcal{D}$ that differ in the data of a single individual, the following inequality holds:
\[
D_{\alpha}(\mathcal{M}(x) \| \mathcal{M}(y)) \leq \varepsilon,
\]
where $D_{\alpha}(P \| Q)$ denotes the Rényi divergence of order $\alpha > 1$ between two probability distributions $P$ and $Q$ defined over a common domain $\mathcal{R}$, given by
\[
D_{\alpha}(P \| Q) = \frac{1}{\alpha - 1} \ln \mathbb{E}_{x \sim Q} \left[ \left( \frac{P(x)}{Q(x)} \right)^{\alpha} \right].
\]
\end{definition}

We rely on the ability of RDP to express privacy loss as a function of the Rényi divergence order $\alpha$, which facilitates accurate accounting and optimization. The RDP formalism allows privacy guarantees to be composed additively and then converted into standard $(\varepsilon, \delta)$-DP bounds.

\begin{theorem}[Composition and Conversion of RDP {\citep[Props. 1 \& 3]{mironov2017renyi}}]\label{the:rdp_conversion}
\leavevmode
\begin{itemize}
    \item[(i)] \textbf{(Additive Composition)}: If mechanisms $\mathcal{M}_1, \ldots, \mathcal{M}_k$ each satisfy $(\alpha, \varepsilon_i)$-RDP, then their composition satisfies $(\alpha, \sum_{i=1}^k \varepsilon_i)$-RDP.
    \item[(ii)] \textbf{(Advanced Composition)}: Let $\mathcal{M}$ be the adaptive composition of $k$ mechanisms, each satisfying $(\alpha, \varepsilon)$-RDP. Then, for any adjacent datasets $x$ and $y$ and any measurable subset $S$ of the output space,
    \[
    \Pr[\mathcal{M}(x) \in S] \leq \exp\left(2 \varepsilon \sqrt{k \log \frac{1}{\Pr[\mathcal{M}(y) \in S]}}\right) \cdot \Pr[\mathcal{M}(y) \in S].
    \]
    \item[(iii)] \textbf{(Conversion)}: If a mechanism satisfies $(\alpha, \varepsilon(\alpha))$-RDP, then for all $0 < \delta < 1$, it also satisfies $(\varepsilon', \delta)$-DP with:
    \[
    \varepsilon' = \varepsilon(\alpha) + \frac{\log(1/\delta)}{\alpha - 1}.
    \]
\end{itemize}
\end{theorem}

These properties enable the use of an RDP accountant to track cumulative privacy loss symbolically and guide parameter selection, which is particularly suitable for DP-SGD. In this context, each iteration of DP-SGD corresponds to applying a Gaussian mechanism to a random mini-batch, and thus can be viewed as a subsampled mechanism. By tracking the Rényi divergence at order $\alpha$ for each step and computing the privacy cost as $\alpha / (2\sigma^2)$ per application of the Gaussian mechanism \citep{mironov2017renyi}, the RDP accountant then applies Theorem~\ref{the:rdp_conversion} to provide tight end-to-end privacy guarantees over the course of training. Unlike classical composition results or the Moments Accountant \citep{abadi2016deep}, the RDP formalism supports symbolic composition of heterogeneous mechanisms and subsampling schemes, offering both generality and computational efficiency. Consequently, it has become a standard tool in privacy-preserving DL and forms the analytical foundation for the experiments in this study.

Building on these theoretical foundations, DP-SGD becomes a highly effective method for AD tasks, particularly in settings involving sensitive training data. Its compatibility with deep AD models, such as autoencoders introduced in Section~\ref{s:background_ae}, stems from the fact that privacy can be enforced by modifying only the optimization procedure, specifically, by incorporating per-example gradient clipping and Gaussian noise, without altering the model architecture or loss function. This enables the training of privacy-preserving AD systems that identify outliers through reconstruction errors while providing rigorous DP guarantees for the underlying data.

\subsection{Autoencoders} \label{s:background_ae}

Autoencoders are a class of neural networks designed for unsupervised learning, originally developed for nonlinear dimensionality reduction and data reconstruction. The two main components include an encoder function that compresses high-dimensional input data $\mathbf{x} \in \mathbb{R}^{m \times d}$ into a low-dimensional code $\mathbf{z} \in \mathbb{R}^{m \times k}$, and a decoder function that uses this code to reconstruct the original input $\hat{\mathbf{x}}$ \citep{hinton2006reducing, sakurada2014anomaly}.

Activation functions play a central role in the architecture of autoencoders by introducing non-linearity into both the encoder and decoder networks. This non-linearity enables the model to learn complex, non-linear mappings between the input space and the latent representation, which would not be possible with purely linear transformations. Given a hidden layer output defined as $\mathbf{a}^{(l)} = \phi(\mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)})$, where $\mathbf{W}^{(l)}$ and $\mathbf{b}^{(l)}$ denote the weight matrix and bias vector, and $\phi(\cdot)$ is the element-wise activation function. The choice of $\phi$ governs the expressiveness and inductive biases of the network. Common choices include the rectified linear unit (ReLU), $\phi(x) = \max(0, x)$, in the hidden layers for its computational efficiency and sparse activations. In the output layer, the activation function is typically selected to match the reconstruction target: for real-valued outputs, a linear activation $\phi(x) = ax$ is used, where $a$ is a constant of choice, whereas for binary outputs, a sigmoid activation $\phi(x) = 1 / (1 + e^{-x})$ maps predictions to the interval $[0, 1]$ and supports a probabilistic interpretation. The activation function must therefore be consistent with the statistical nature of the data \citep{sharma2017activation}.

By minimizing the discrepancy between the input $\mathbf{x}$ and its reconstruction $\hat{\mathbf{x}}$ during training, autoencoders learn efficient and compact representations of the data. The loss function used to train autoencoders typically consists of a reconstruction error term that measures how well the output matches the input, along with a L2 regularization term to reduce overfitting. The general form of the objective function is given by:
\begin{align} \label{eq:loss_func}
J(\mathbf{W}, \mathbf{b}) = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(\mathbf{x}^{(i)}, \hat{\mathbf{x}}^{(i)}) + \frac{\lambda}{2} \sum_{l=1}^{L-1} \sum_{j=1}^{s_l} \sum_{k=1}^{s_{l+1}} \left( W_{kj}^{(l)} \right)^2,
\end{align}
where $\mathcal{L}(\mathbf{x}^{(i)}, \hat{\mathbf{x}}^{(i)})$ is the per-example reconstruction loss, $m$ is the number of training samples, $\lambda$ is the regularization coefficient, $L$ is the total number of layers, and $s_l$ is the number of units in layer $l$ \citep{sakurada2014anomaly}. This objective is typically minimized using SGD or one of its variants, with gradients computed via backpropagation.

The design of the reconstruction loss is typically feature-specific and depends on the type of input data. For real-valued variables, loss functions such as mean squared error (MSE) is commonly used, as it penalizes the squared deviation between the predicted and observed values \citep{sakurada2014anomaly}. In contrast, binary variables are typically modeled using cross-entropy loss, which measures the discrepancy between the predicted probabilities and the actual binary outcomes under a Bernoulli likelihood \citep{hinton2006reducing}. These loss functions align with the statistical nature of each feature type and form the basis of the reconstruction objective in mixed-type settings. The precise formulation and implementation are detailed in Section~\ref{s:method}.

Autoencoders are widely used for AD by leveraging their ability to learn compressed representations of normal data through nonlinear dimensionality reduction. During training, the autoencoder is exposed only to normal samples, learning to reconstruct the input data with minimal error. The inference set of labeled data are then passed through the network, and the anomaly error is evaluated using the anomaly score. Since autoencoders are optimized to reconstruct data that conforms to the learned distribution, they typically produce low errors for normal inputs and significantly higher errors for anomalous ones \citep{sakurada2014anomaly}.

\subsection{eXplainable AI}

\iffalse\begin{itemize}
    \item Theoretical background on SHAP, LIME, and feature attribution methods.
    \item How privacy affects interpretability in DL models.
    \item Why KernelSHAP is chosen and its relevance to DP models.
\end{itemize}\fi