\begin{algorithm}[!t]
\footnotesize
\caption{DP-SGD autoencoder training} \label{alg:dp_sgd}
\begin{algorithmic}[1]
\REQUIRE Training data $\mathbf{x} = \{\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(m)}\}$, learning rate $\alpha$, regularization $\lambda$, max epochs $E$, patience $p$, lot size $L$, clipping norm $C$, noise multiplier $\sigma$
\ENSURE Trained weights $\mathbf{W}^{(l)}$, biases $\mathbf{b}^{(l)}$

\STATE Initialize $\mathbf{W}^{(l)}, \mathbf{b}^{(l)}$ randomly
\STATE Split data: $\mathbf{x}_{\text{train}}, \mathbf{x}_{\text{val}}$; set $best\_val\_loss \leftarrow \infty$, $patience \leftarrow 0$

\FOR{epoch $= 1$ to $E$}
    \STATE Compute sampling probability $q \leftarrow L / |\mathbf{x}_{\text{train}}|$
    \STATE Shuffle $\mathbf{x}_{\text{train}}$
    \FOR{each training step in epoch}
        \STATE Randomly sample lot $\mathcal{L}_t \subset \mathbf{x}_{\text{train}}$ with sampling probability $q$
        \FOR{each example $\mathbf{x}^{(i)} \in \mathcal{L}_t$}
            \STATE \textbf{Forward pass:} propagate inputs through encoder and decoder using ReLU activations in hidden layers; apply mixed output activations (sigmoid for $\hat{\mathcal{B}}_{j,\mathcal{B}}$, linear for $\hat{\mathcal{B}}_{j,\mathcal{R}}$) to compute reconstruction $\hat{\mathcal{B}}_j$
            \STATE \textbf{Loss:} compute $\mathcal{L}(\mathcal{B}_j)$ as a weighted combination of MSE for $\mathcal{R}$ (Eq.~\ref{eq:mse}) and cross-entropy for $\mathcal{B}$ (Eq.~\ref{eq:cee}), plus regularization
            \STATE \textbf{Gradient:} compute $\nabla^{(i)}_{\mathbf{W}^{(l)}}, \nabla^{(i)}_{\mathbf{b}^{(l)}}$
            \STATE \textbf{Clip:} $\bar\nabla^{(i)} \leftarrow \nabla^{(i)} / \max\left(1, \frac{\|\nabla^{(i)}\|_2}{C}\right)$
        \ENDFOR
        \STATE \textbf{Aggregate and add noise:} $\tilde{\nabla} \leftarrow \frac{1}{L} \sum_i(\bar\nabla^{(i)} + \mathcal{N}(0, \sigma^2 C^2 \mathbf{I}))$
        \STATE \textbf{Update:} $\mathbf{W}^{(l)} \leftarrow \mathbf{W}^{(l)} - \alpha \cdot \tilde{\nabla}_{\mathbf{W}^{(l)}}$ and similarly for $\mathbf{b}^{(l)}$
    \ENDFOR

    \STATE \textbf{Validation:} compute average loss $\mathcal{L}_{\text{val}}$ on $\mathbf{x}_{\text{val}}$
    \IF{$\mathcal{L}_{\text{val}} < best\_val\_loss$}
        \STATE $best\_val\_loss \leftarrow \mathcal{L}_{\text{val}}$, $patience \leftarrow 0$
    \ELSE
        \STATE $patience \leftarrow patience + 1$
        \IF{$patience \geq p$}
            \STATE \textbf{Break} (early stop)
        \ENDIF
    \ENDIF
\ENDFOR

\RETURN $\{\mathbf{W}^{(l)}, \mathbf{b}^{(l)}\}$
\end{algorithmic}
\end{algorithm}