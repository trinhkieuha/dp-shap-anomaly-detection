\subsection{Model Performance} \label{s:model_perf}

This subsection presents the predictive performance of all models evaluated in this study, with a focus on their ability to distinguish anomalies from normal observations. The evaluation proceeds in two stages. First, we analyze the validation performance of the baseline autoencoder under four distinct hyperparameter tuning objectives, precision, recall, F1-score, and AUC, using the results of the Bayesian optimization procedure. These configurations serve as a basis for understanding trade-offs across performance metrics. To ensure consistency in downstream comparisons, one tuning objective will later be selected as a common basis for evaluating the final configurations of all models, including those incorporating DP-SGD and post-hoc noise mechanisms. 

\subsubsection{Baseline Autoencoders}

To begin the analysis, we evaluate the performance of the baseline autoencoder models on the validation set. Table~\ref{tab:baseline-validation} presents the results obtained under four distinct tuning objectives: AUC, precision, recall, and F1-score.

Overall, each modelâ€™s performance reflects the emphasis of its corresponding tuning objective, with the optimized metric generally reaching its highest value under that criterion. This consistency supports the effectiveness of the Bayesian optimization procedure and provides a principled foundation for subsequent evaluation and model selection. A slight deviation from this pattern occurs in the AUC-optimized model, which achieves a marginally higher validation F1-score ($54.08\%$) than the model tuned directly for F1-score ($53.72\%$). While the difference is minimal, it suggests that AUC-based tuning can be competitive, even for F1-based evaluation. This result may reflect the benefit of the two-stage AUC-driven strategy, where class separability is optimized during training and the classification threshold is adjusted post hoc to improve F1-score.

\subfile{tables/baseline_validation}

The validation results demonstrate clear trade-offs across tuning objectives, where improvements in one metric often come at the expense of others. The model optimized for precision achieves the highest precision at $77.14\%$, but this comes with a sharp decline in recall to just $19.18\%$, resulting in a substantially lower F1-score of $30.72\%$. This outcome reflects a conservative decision boundary that prioritizes correctness over coverage. In comparison, the recall-optimized model exhibits a more balanced performance, achieving the highest recall at $64.51\%$ while maintaining the lowest, yet still acceptable, precision at $43.26\%$. This leads to a strong F1-score of $51.79\%$ and AUC of $79.63\%$. These results suggest that tuning for recall can offer robust overall performance, particularly in settings where missing true anomalies is more costly than tolerating false positives.

Additionally, both F1-score and AUC emerge as viable tuning criteria due to their ability to capture the trade-off between sensitivity and specificity in a balanced manner. While the AUC-tuned model achieves the highest validation F1-score and AUC, along with the second-highest precision ($49.61\%$) and recall ($68.96\%$), the differences in F1-score and AUC between the recall-, F1-, and AUC-optimized models are relatively small. The marginal differences suggest that all three objectives yield comparable and robust performance, thereby justifying their consideration as potential candidates for selecting the final model. The final choice of tuning objective, therefore, will also take into account the behavior of models trained with DP, as it is important to ensure a consistent and fair basis for comparison across all privacy mechanisms and configurations.

\subsubsection{Output-Perturbed Autoencoders}

\subsubsection{DP-SGD Autoencoders}

\subfile{tables/dpsgd_validation}

\iffalse\begin{itemize}
	\item State chosen $\varepsilon$ values; describe privacy-performance behavior.
	\item Refer to noise accounting files and tuning results.
	\item Optional: Highlight best $\varepsilon$ and headline test metric (e.g., recall improvement or precision drop).
\end{itemize}\fi

\subsubsection{Performance Comparison across Privacy Mechanisms}
\iffalse\begin{itemize}
	\item Main performance table (Table II): Compare Accuracy, Precision, Recall, F1, AUC across all models.
	\item Visual comparison (bar chart or radar plot).
	\item Comment briefly but neutrally on the overall results.
\end{itemize}\fi

\subsection{Explainability}

\subsubsection{Baseline Autoencoders}

\subsubsection{DP-SGD Autoencoders}

\subsubsection{Output-Perturbed Autoencoders}

\subsubsection{Summary of Explainability}