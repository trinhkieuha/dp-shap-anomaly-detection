% DP in AD
DP has been increasingly integrated into deep AD to enhance privacy protection while maintaining model utility. Although the literature lacks a comprehensive survey dedicated exclusively to DP-AD, several studies have proposed innovative approaches that incorporate DP within AD frameworks across various domains. For instance, \cite{fan2013differentially} introduce a real-time AD method under DP, leveraging FAST filtering and practical sensitivity analysis for epidemic outbreak detection. Similarly, \cite{kurt2022online} present an online AD framework that applies DP within generalized CUSUM detection in distributed networks, ensuring privacy-preserving anomaly score aggregation with theoretical guarantees on false alarm rates and detection delays.

% DP in deep-AD
More recent efforts shift toward DL-based AD. For example, \cite{du2019robust} explore DP-AD by integrating autoencoders and long short-term memory (LSTM) networks, demonstrating its efficacy in Hadoop log AD and backdoor attack defense while preserving model performance. Other approaches enhance detection mechanisms, such as log-likelihood ratio tests for adversarial robustness \cite{giraldo2020adversarial}, private outlier queries that improve utility through local sensitivity and an exponential mechanism \cite{okada2015differentially}, and DP-SGD frameworks that optimize privacy budget consumption while ensuring model accuracy in fraud detection \cite{wang2018privacy}. In a distinct post-hoc direction, \cite{lu2022differentially} inject calibrated noise into a randomly selected output neuron at inference time, enabling black-box privacy without modifying the training process.

% DP in DL - trade-off
The integration of DP into DL models introduces inherent trade-offs between privacy protection and model utility. While adding DP noise safeguards sensitive data, it often comes at the cost of degraded model accuracy. In the context of machine learning, \cite{jayaraman2019evaluating} reveal that strict DP settings (low $\varepsilon$) severely degrade accuracy, while relaxed settings (high $\varepsilon$) increase privacy leakage, as demonstrated through membership and attribute inference attacks. In addition, \cite{wu2020value} derive a privacy-utility trade-off that accuracy loss is inversely proportional to the square of both $\varepsilon$ and dataset size. Their study also proposes mitigating these effects with DP-SGD variants that retain up to 90\% of non-private model performance in fraud detection. Most notably, \cite{zhao2020not} show that applying DP noise later in the machine learning pipeline preserves better utility, with accuracy loss influenced more by dataset complexity and machine learning method than by the privacy budget ($\varepsilon$).

% DP in AD - trade-off
Among machine learning applications, the challenges of integrating DP in AD models are particularly pronounced due to the fundamental conflict between outlier detection and individual privacy preservation. \cite{okada2015differentially} show that global sensitivity-based DP mechanisms introduce excessive noise in high-dimensional data, significantly impairing outlier detection accuracy, while their smooth sensitivity approach improves utility but remains constrained by strict privacy settings. Similarly, \cite{fan2013differentially} demonstrate that adding Laplace noise to the input of the Kalman filter in epidemic outbreak detection degrades the anomaly identification ability, especially when the dataset displays high variability. \cite{kurt2022online} show that perturbing local data processing in network monitoring obscures subtle deviations, increasing false alarms and delaying detection under strict privacy constraints. In the context of DL, \cite{wang2018privacy} find that DP noise in DP-SGD disrupts training in deep neural networks, leading to unstable optimization and reduced model accuracy in fraud detection.  Beyond these issues, \cite{giraldo2020adversarial} warn that adversaries can exploit DP noise to make malicious data resemble normal behavior, evading detection in traffic congestion and smart grid monitoring. Finally, \cite{du2019robust} illustrate how DP degrades the modelâ€™s ability to capture fine-grained anomaly patterns, increasing false negatives, particularly in backdoor attack detection.

% XAI in AD - survey
Explainability in AD has recently gained increasing research attention, as prior literature has predominantly prioritized enhancing detection performance over interpretability. Comprehensive surveys, such as \cite{panjei2022survey}, highlight key challenges in explainable AD, including standardizing outlier scores, handling high-dimensional data ($d > n$), and identifying causal relationships among anomalies. Similarly, \cite{li2023survey} review post-hoc explainability methods, emphasizing the widespread use of SHAP in DL models, such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), and autoencoders (AEs),  despite high computational costs.

% XAI in AD - specific models
Beyond general surveys, several studies have explored specific explainable AD models, collectively highlighting the growing role of XAI in AD. \cite{giurgiu2019additive} extend KernelSHAP for temporal AD by identifying both contributing and counteracting signals, demonstrating their approach in epileptic seizure detection and storage performance monitoring. \cite{chawla2020interpretable} demonstrate the effectiveness of Sparse Autoencoders (SAEs) combined with SHAP in mobile network AD, enabling actionable insights for network operators and improving automated troubleshooting in telecom applications. Additionally, \cite{serradilla2021adaptable} evaluate a range of AD models and integrate SHAP-based interpretability techniques into the best-performing model, a two-dimensional convolutional autoencoder (2D-CNN-AE), for failure diagnosis and transfer learning. In the domain of network security, \cite{roshan2021utilizing} utilize SHAP for feature selection in network security, improving autoencoder-based AD to achieve 94\% accuracy and an AUC of 0.969. Lastly, \cite{wu2022explainable} introduce the DAE-SHAP framework, which integrates pre-trained models for feature extraction, achieving state-of-the-art accuracy on the UCSD Pedestrian dataset with minimal training time.

% DP-XAI - trade-off
While these explainability techniques enhance AD models, integrating DP into XAI introduces new challenges, particularly in the trade-offs between privacy and interpretability. \cite{naidu2021differential} demonstrate that DP-SGD with strict privacy constraints (low $\varepsilon$) severely degrades interpretability by making class activation maps (CAMs) less informative, a critical limitation in medical AI where both privacy and explainability are essential. Similarly, \cite{bozorgpanah2022privacy} show that privacy-preserving techniques, such as masking and noise addition, distort Shapley values, altering feature importance rankings and reducing the reliability of explanations, particularly in linear models. Extending this analysis, \cite{bozorgpanah2024explainable} introduce the Irregularity metric to quantify how k-anonymity via microaggregation (MDAV) and DP via Laplace noise impact SHAP-based attributions, highlighting the intricate trade-offs between privacy and interpretability. Furthermore, \cite{patel2022model} demonstrate that DP significantly diminishes the fidelity of post-hoc explanations, with stricter privacy constraints leading to substantial distortions in feature importance rankings. While adaptive DP mechanisms help mitigate these effects, maintaining both strong privacy guarantees and reliable explanations remains a persistent challenge. These findings underscore the need for further research to refine DP-XAI methods, ensuring robust model interpretability while complying with privacy regulations.

% DP-XAI - methods
Given these challenges, recent research has focused on developing DP-XAI mechanisms that enhance interpretability while ensuring privacy protection. \cite{harder2020interpretable} introduce Locally Linear Maps (LLM), a privacy-preserving model that approximates complex classifiers with locally linear functions per class. By injecting noise into gradient updates, LLM ensures DP while preserving both local and global explainability. Similarly, \cite{jetchev2023xorshap} develop XorSHAP, the first privacy-preserving SHAP algorithm for decision tree ensembles, leveraging Secure Multiparty Computation (SMPC) to compute feature attributions without exposing sensitive data. Through oblivious operations and parallel processing, XorSHAP improves efficiency while ensuring compliance with GDPR and the EU AI Act. Focusing on local DP, \cite{nguyen2023xrand} propose XRAND, a mechanism that obfuscates top-k important features using a two-step randomized response (RR) approach, mitigating inference attacks while preserving explanation faithfulness. Lastly, \cite{patel2022model} design an adaptive DP algorithm that dynamically allocates the privacy budget across multiple explanation requests. By reusing previously computed explanations, their method minimizes redundant noise injection, maintaining fidelity while ensuring strong privacy protection.

% DP-XAI in AD
The integration of XAI and DP in deep AD remains a critical yet underexplored research frontier. \cite{saifullah2022privacy} is among the first to provide a comprehensive survey of the interpretability-utility-privacy (IUP) trade-off using different privacy-preserving techniques in DL. Part of their study specifically examines how DP affects the quality of explanations generated by XAI methods in DL-based AD. The study evaluates the trade-off by applying DP to various deep models trained on AD datasets and analyzing the impact on different attribution methods, including gradient-based and perturbation-based explainability techniques. Their findings reveal that DP significantly degrades both the fidelity and interpretability of explanations, introducing noise into attribution maps and reducing their reliability for understanding model decisions. Another study, \cite{ezzeddine2024differential}, experimentally evaluates this trade-off using SHAP to analyze the impact of DP on detection accuracy and explainability. The findings indicate that enforcing DP significantly reduces both, with a notable decline in performance metrics and model interpretability. The statistical analysis confirms the significance of this trade-off ($p$-value $< 0.05$), underscoring the challenges of deploying privacy-preserving XAI models.

% Research gaps
While these studies provide valuable insights into the IUP trade-off, important gaps remain. Since \cite{saifullah2022privacy} examine the IUP trade-off across various domains, they do not focus on the unique challenges of deep AD, such as high-class imbalance and subtle anomaly patterns. Additionally, although DP is known to reduce model explainability, its specific trade-offs in AD detection performance have not been quantified. These gaps are addressed by \cite{ezzeddine2024differential}, which focuses on AD and quantifies the trade-off by varying privacy budgets ($\varepsilon$). However, the study relies on simple machine learning models, specifically iForest and LOF, which may not capture complex data patterns as effectively as DL approaches. With the increasing adoption of DL techniques like autoencoders with XAI for AD \citep{li2023survey}, further research is needed to understand how privacy constraints impact their performance and interpretability.

% Sometimes, you want to cite in parentheses, like this \citep[see the last page of][pg. 1033]{Arnade}.