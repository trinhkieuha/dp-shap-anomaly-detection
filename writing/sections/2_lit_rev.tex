% DP in AD
DP has been increasingly integrated into deep AD to enhance privacy protection while maintaining model utility. Early works adapt statistical detectors: \cite{fan2013differentially} add Laplace noise to FAST filtering for real‑time epidemic monitoring, and \cite{kurt2022online} apply $\varepsilon$‑DP to distributed CUSUM aggregation with bounded false‑alarm rates and detection delays. Recent DL‑based AD approaches embed DP‑SGD in AE–LSTM architectures for Hadoop log AD and backdoor defense \citep{du2019robust}, utilize private outlier queries via smooth sensitivity and exponential mechanisms \citep{okada2015differentially}, and optimize fraud‑detection models under DP‑SGD \citep{wang2018privacy}, while post‑hoc methods like \citep{lu2022differentially} inject calibrated noise into a single output neuron at inference for black‑box privacy without retraining.

% DP in DL and AD – trade‑off
Integrating DP into DL and AD models entails a fundamental trade‑off between privacy and performance: DP noise protects sensitive data but degrades accuracy. In DL, strict budgets (low $\varepsilon$) drastically reduce accuracy while relaxed budgets (high $\varepsilon$) increase privacy leakage via inference attacks \citep{jayaraman2019evaluating}. The stage where noise is injected exerts a significant influence on the privacy–performance trade‑off, Empirical results on the optimal timing of noise injection are mixed. \cite{wu2020value} report that DP‑SGD variants, despite accuracy loss scaling inversely with $\varepsilon^2$ and dataset size, can recover up to 90\% of non‑private performance, whereas \cite{zhao2020not} find that deferring noise injection to later pipeline stages further mitigates utility degradation, making accuracy depend more on dataset complexity and model architecture than on $\varepsilon$. In AD, DP’s conflict with outlier detection is more acute: global-sensitivity mechanisms add prohibitive noise in high dimensions \citep{okada2015differentially}, and Laplace perturbations in Kalman filters impair anomaly identification under high variability \citep{fan2013differentially,kurt2022online}. In DL‑based AD, DP‑SGD destabilizes optimization and reduces detection accuracy \citep{wang2018privacy,du2019robust}, while adversaries may exploit DP noise to camouflage malicious behavior \citep{giraldo2020adversarial}.

% XAI in AD
Interest in XAI for AD has surged as interpretability emerges alongside performance. Surveys identify key challenges, including standardizing outlier scores, managing high-dimensional data ($d>n$), and uncovering causal anomaly relations \citep{panjei2022survey}, and review post-hoc methods like SHAP in DL models (RNNs, CNNs, AEs) despite their computational overhead \citep{li2023survey}. The combination of autoencoders and SHAP has become a common paradigm for explainable AD. For instance, \cite{giurgiu2019additive} extend KernelSHAP for temporal AD in seizure detection and system monitoring, \cite{chawla2020interpretable} combine Sparse AEs with SHAP for mobile network troubleshooting, and \cite{serradilla2021adaptable} integrate SHAP into a 2D‑CNN‑AE for failure diagnosis and transfer learning.

% DP-XAI - trade-off
While these explainability techniques enhance AD models, integrating DP into XAI introduces new challenges, particularly in the trade-offs between privacy and interpretability. \cite{naidu2021differential} show that DP‑SGD under strict budgets (low $\varepsilon$) renders CAMs uninformative, severely undermining interpretability in medical AI. \cite{bozorgpanah2022privacy} find that masking and noise addition distort Shapley values, skewing feature importance in linear models, while \cite{bozorgpanah2024explainable} propose an Irregularity metric to quantify the impacts of MDAV k‑anonymity and Laplace DP noise on SHAP attributions. \cite{patel2022model} further demonstrate that tighter privacy constraints degrade post‑hoc explanation fidelity. Although adaptive DP schemes alleviate some distortions, achieving both rigorous privacy and reliable explanations remains an open challenge requiring dedicated DP‑XAI research.

% DP-XAI in AD
Although DP‑XAI trade‑offs have been explored in general ML settings, few studies specifically examine these challenges within AD. \cite{saifullah2022privacy} is among the first to provide a comprehensive survey of the interpretability-utility-privacy (IUP) trade-off using different privacy-preserving techniques in DL. Part of their study specifically examines how DP affects the quality of explanations generated by XAI methods in DL-based AD. The study evaluates the trade-off by applying DP to various deep models trained on AD datasets and analyzing the impact on different attribution methods, including gradient-based and perturbation-based explainability techniques. Their findings reveal that DP significantly degrades both the fidelity and interpretability of explanations, introducing noise into attribution maps and reducing their reliability for understanding model decisions. Another study, \cite{ezzeddine2024differential}, experimentally evaluates this trade-off using SHAP to analyze the impact of DP on detection accuracy and explainability. The findings indicate that enforcing DP significantly reduces both, with a notable decline in performance metrics and model interpretability. The statistical analysis confirms the significance of this trade-off ($p$-value $< 0.05$), underscoring the challenges of deploying privacy-preserving XAI models.

% Research gaps
While these studies provide valuable insights into the IUP trade-off, important gaps remain. Since \cite{saifullah2022privacy} examine the IUP trade-off across various domains, they do not focus on the unique challenges of deep AD, such as high-class imbalance and subtle anomaly patterns. Additionally, although DP is known to reduce model explainability, its specific trade-offs in AD detection performance have not been quantified. These gaps are addressed by \cite{ezzeddine2024differential}, which focuses on AD and quantifies the trade-off by varying privacy budgets ($\varepsilon$). However, the study relies on simple machine learning models, specifically iForest and LOF, which may not capture complex data patterns as effectively as DL approaches. With the increasing adoption of DL techniques like autoencoders with XAI for AD \citep{li2023survey}, further research is needed to understand how privacy constraints impact their performance and interpretability.

In light of the predominant use of autoencoder‐based architectures in conjunction with KernelSHAP for explainable anomaly detection \citep{giurgiu2019additive,chawla2020interpretable,serradilla2021adaptable}, this study adopts a deep autoencoder coupled with KernelSHAP as its principal explainability framework. While \cite{saifullah2022privacy} offer a comprehensive survey of the interpretability–utility–privacy (IUP) trade‑off in DL‑based AD, they neither address domain‑specific challenges such as high class imbalance and subtle anomaly patterns, nor systematically vary the privacy budget. Conversely, \cite{ezzeddine2024differential} quantify the IUP trade‑off by varying $\varepsilon$ and specifically address AD, but their experiments are limited to classical ML models (iForest, LOF) and thus may not generalize to deep architectures. Moreover, mixed findings regarding the optimal point of DP noise injection, training‑time DP‑SGD \citep{wang2018privacy, wu2020value} versus post‑hoc perturbation \citep{zhao2020not,lu2022differentially}, have yet to be reconciled in the AD context. Accordingly, we implement and directly compare both noise‑injection strategies under consistent conditions, and systematically evaluate a spectrum of privacy budgets ($\varepsilon$) ranging from stringent to permissive, thereby delineating the privacy–interpretability frontier in deep AD.